Here are some instructions on compiling lmf for MPI written originally
by Tony Paxton. For now this is rather crude. Later we will build the
options into configure.

First you must create a MPI version of slatsm (slatsm-MPI.a).
See slatsm/startup/MPI-instructions.

You may at this stage want to test your system before trying to run
lmf.  The program mm.f in lm-x.x/doc is the simple matrix multiplier
described in "Using MPI" by Gropp et al. 2nd ed., MIT Press. tpzhev.f
tests the parallel diagonaliser pzhev.f which is in the lm-x.x/fp
directory. To run mm.f you need to compile it with your system's
mpif90 (or equivalent, see above) and link it using mpif90 (or
equivalent) with slatsm-MPI.a. To run a parallel calculation, you need,
as a bare minimum, mpirun and a machine file containing the names
of machines you want to run on. But this should not crop up here. By now
you have either installed mpich and run some test examples, or have read
the documentation on how to submit a job to your cluster using mpimon
or some queuing software like PBS or NQS.

Note a few points about mm.f
1) You can test it on increasing numbers of processors to assess its
   scalability. The master process does no calculation: it merely
   handles the distribution of work to the slaves and gather results
   (hence only a demonstration program) so using 2,3,5,9 ... processors
   tests scaling of theoretical speedup of 1,2,4,8 ...

2) It uses FORTRAN 90. The MPI extensions to lmf are in F90. However as
   long as all MPI extensions are commented out by ccomp lmf is still
   F77 standard. F90 allocate is useful, but used minimally in lmf.
   Large buffers for communication are still taken from the heap.

To compile and link tpzhev.f you will need BLACS and SCALAPACK. If you
don't have these or don't want to install them (netlib.org) then don't
ccomp pzhev.f in lm-x.x/fp (see below) -- leave it as it is and do not
invoke lmf --pdiag which calls the parallel diagonaliser.

Now, for lmf.  There is a shell script that will compile appropriate
subroutine libraries for you.  As when compiling slatsm-MPI.a, you
first need to do the following:

1. Create an MPI version of slatsm.a
   (See slatsm/startup/MPI-instructions in the slatsm directory)

2. Set environment variable F90M to point to
   an appropriate MPI-f90 compiler, followed by switches, e.g.
   F90M=/usr/local/mpich-1.2.5_shared/bin/mpif90 -I/usr/local/mpich-1.2.5_shared/include -cm -O3 -xW -ip

3. Set these variables in Make.inc:   F90M, LIBSLA_MPI, LIBLOC_MPI

4. Make sure preprocessor ccomp is in your path, or
   set environment variable CCOMP pointing to ccomp.

5. From the top-level directory, invoke :
     cd subs
     ../startup/subs-to-mpi
     ../startup/subs-to-mpik
     cd ../fp
     ../startup/subs-to-mpi
     ../startup/subs-to-mpik
     cd ..

This will create subs-MPI.a and subs-MPIK.a and make MPI variants of any fortran source
files that need it in subs and fp libraries.

6. Remember, if you don't have BLACS and SCALAPACK, exclude pzhev.f from
   this operation:
     ar dv subs-MPI.a pzhev-MPI.o
     ar xv subs.a pzhev.o
     ar rv subs-MPI.a pzhev.o
     rm pzhev.o

7.  In the top-level directory, create the object file(s)

       ccomp -dMPI -uMPE lmf.f lmf-MPI.f
       ccomp -dMPIK -uMPE lmf.f lmf-MPIK.f
       $F90M -c lmf-MPI.f
       $F90M -c lmf-MPIK.f


8.  Add to Makefile the lines below:

lmf-MPI:	lmf-MPI.o $(SUBS)
	$(F90M) $@.o $(SUBS) fp/subs-MPI.a subs/subs-MPI.a $(LIBES_MPI) $(LKFLAGS2) -o $@

lmf-MPIK:	lmf-MPIK.o $(SUBS)
	$(F90M) $@.o $(SUBS) fp/subs-MPIK.a subs/subs-MPI.a $(LIBES_MPI) $(LKFLAGS2) -o $@

Link lmf-MPI with 'make -f Makefile lmf-MPI'

Now try and link lmf-MPI.o. You should first look at how 'make' did this
when it linked lmf.o; then cut and paste the linking line and replace
slatsm.a with slatsm-MPI.a, subs.a with subs-MPI.a and include any other
links such as BLACS and SCALAPACK. Note that your local version of
mpif90 (or equivalent) will link the MPI libraries, but again you can
try

f90 -L/opt/scali/lib

or

f90 -L/usr/local/mpich-1.2.5/lib

Now you should be able to invoke

mpirun -np # -machinefile file lmf-MPI [lmf-args] ext

Next you can repeat the whole sequence, now creating subs-MPIK.a in
subs and fp directories instead of subs-MPI.a.  This implementation
that parallelises the k-loop. This will be more useful if you have a
smallish unit cell and several k-points. Here it makes sense to the
choose a number of processors that divides evenly into the number of
k-points.

Be sure to compare the output, bands, total energy etc. with a serial
calculation and report bugs to Tony.Paxton@QUB.ac.uk
